# Configuration file for training GPT with MoE

# I/O
out_dir: 'out'
eval_interval: 2000
log_interval: 10
eval_iters: 200
eval_only: False  # If True, script exits right after the first eval
always_save_checkpoint: True  # If True, always save a checkpoint after each eval
init_from: 'scratch'  # 'scratch' or 'resume' or 'gpt2*'

# WandB logging
wandb_log: False  # Disabled by default
wandb_project: 'owt'
wandb_run_name: 'gpt2'

# Data
dataset: 'shakespeare'
gradient_accumulation_steps: 40  # Used to simulate larger batch sizes
batch_size: 12  # If gradient_accumulation_steps > 1, this is the micro-batch size
block_size: 1024

# Model
n_layer: 12
n_head: 12
n_embd: 768
dropout: 0.0  # For pretraining 0 is good, for finetuning try 0.1+
bias: False  # Do we use bias inside LayerNorm and Linear layers?

# MoE parameters
use_moe: True
num_experts: 4
num_experts_per_tok: 2
moe_loss: True
moe_loss_type: 'entropy_regularization'
moe_loss_coef: 0.01

# AdamW optimizer
learning_rate: 6e-4  # Max learning rate
max_iters: 600000  # Total number of training iterations
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0  # Clip gradients at this value, or disable if == 0.0

# Learning rate decay settings
decay_lr: True  # Whether to decay the learning rate
warmup_iters: 2000  # How many steps to warm up for
lr_decay_iters: 600000  # Should be ~= max_iters per Chinchilla
min_lr: 6e-5  # Minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# DDP settings
backend: 'nccl'  # 'nccl', 'gloo', etc.

# System
device: 'cuda'  # Examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.
dtype: 'bfloat16'  # 'float32', 'bfloat16', or 'float16'
compile: False  # Use PyTorch 2.0 to compile the model to be faster
